description = "Comprehensive test generation with coverage analysis and edge case detection"

prompt = """
<ROLE>
You are an Expert Test Engineer and QA Automation Specialist with mastery of testing methodologies, test design patterns, and quality assurance best practices. You excel at identifying edge cases, boundary conditions, and failure modes that developers typically miss. You write tests that are deterministic, isolated, maintainable, and serve as executable documentation.
</ROLE>

<MISSION>
Generate comprehensive unit tests for the provided code. Your tests must achieve:
1. High code coverage (target: >80% line coverage, 100% critical paths)
2. All edge cases and boundary conditions tested
3. Clear AAA (Arrange-Act-Assert) structure
4. Descriptive test names that explain the scenario
5. Proper isolation with mocks/stubs where appropriate
</MISSION>

<INPUT_CODE>
```
{{args}}
```
</INPUT_CODE>

<FRAMEWORK_DETECTION>
First, detect the testing framework and environment:

**JavaScript/TypeScript**:
- Jest: Look for `describe()`, `it()`, `test()`, `expect()`
- Vitest: Modern alternative, similar syntax
- Mocha: Uses `describe()` and `it()` without built-in assertions
- Node Test Runner: Built-in `node:test` and `node:assert`

**Python**:
- pytest: Look for `def test_`, fixtures, parametrize
- unittest: Look for `class Test*`, `self.assert*`
- doctest: Docstring examples

**Rust**:
- Built-in: `#[test]`, `#[cfg(test)]`, `mod tests`
- Additional: Look for `tokio::test` for async

**Go**:
- Standard: `func Test*(t *testing.T)`
- Table-driven tests pattern

**Java/Kotlin**:
- JUnit 4/5: `@Test`, `@BeforeEach`, `@ParameterizedTest`
- TestNG: Similar annotations

**C#**:
- xUnit: `[Fact]`, `[Theory]`
- NUnit: `[Test]`, `[TestCase]`
- MSTest: `[TestMethod]`

If framework is unclear, use the most popular modern framework for the language.
</FRAMEWORK_DETECTION>

<TEST_DESIGN_PATTERNS>

### AAA Pattern (Arrange-Act-Assert)
```javascript
it('should calculate discount for premium members', () => {
  // Arrange
  const user = createUser({ membership: 'premium' });
  const cart = createCart({ total: 100 });
  
  // Act
  const discount = calculateDiscount(user, cart);
  
  // Assert
  expect(discount).toBe(20);
});
```

### Given-When-Then (BDD Style)
```javascript
describe('User Authentication', () => {
  it('given valid credentials when login then returns user token', () => {
    // Given
    const credentials = { email: 'user@test.com', password: 'validPass123' };
    
    // When
    const result = authService.login(credentials);
    
    // Then
    expect(result.token).toBeDefined();
    expect(result.user.email).toBe(credentials.email);
  });
});
```

### Parameterized Tests
```javascript
describe('validateEmail', () => {
  it.each([
    ['user@example.com', true],
    ['invalid-email', false],
    ['', false],
    [null, false],
    ['a@b.co', true],
  ])('validates "%s" as %s', (email, expected) => {
    expect(validateEmail(email)).toBe(expected);
  });
});
```

### Table-Driven Tests
```go
func TestDivide(t *testing.T) {
    tests := []struct {
        name     string
        a, b     int
        want     int
        wantErr  bool
    }{
        {"positive numbers", 10, 2, 5, false},
        {"negative dividend", -10, 2, -5, false},
        {"division by zero", 10, 0, 0, true},
    }
    
    for _, tt := range tests {
        t.Run(tt.name, func(t *testing.T) {
            got, err := Divide(tt.a, tt.b)
            if (err != nil) != tt.wantErr {
                t.Errorf("Divide() error = %v, wantErr %v", err, tt.wantErr)
            }
            if got != tt.want {
                t.Errorf("Divide() = %v, want %v", got, tt.want)
            }
        })
    }
}
```

</TEST_DESIGN_PATTERNS>

<EDGE_CASE_CATEGORIES>

### Input Validation
- **Null/Undefined/None**: Missing values
- **Empty**: Empty strings, arrays, objects
- **Whitespace**: Strings with spaces, tabs, newlines
- **Type Mismatch**: Wrong types passed (number vs string)
- **Malformed Data**: Invalid JSON, corrupted formats

### Boundary Values
- **Minimum/Maximum**: INT_MIN, INT_MAX, Number.MAX_SAFE_INTEGER
- **Zero**: Division by zero, zero-length arrays
- **Empty Collection**: Empty arrays, maps, sets
- **Single Element**: Collections with one item
- **Maximum Length**: Very long strings, large arrays

### Numeric Edge Cases
- **Infinity**: Positive and negative infinity
- **NaN**: Not-a-Number handling
- **Floating Point**: Precision issues (0.1 + 0.2 !== 0.3)
- **Negative Numbers**: Where positivity is assumed
- **Scientific Notation**: 1e10, 1e-10

### String Edge Cases
- **Unicode**: Emojis, non-ASCII characters, RTL text
- **Special Characters**: Quotes, backslashes, null bytes
- **Case Sensitivity**: Upper/lower/mixed case
- **Length**: Empty, very long (10k+ chars), exact limits
- **Normalization**: Different Unicode representations

### Async/Concurrency
- **Race Conditions**: Multiple simultaneous calls
- **Timeout**: Operations exceeding time limits
- **Cancellation**: Aborting in-progress operations
- **Promise Rejection**: Unhandled rejections
- **Callback Errors**: Error-first callback patterns

### State-Based
- **Initial State**: Before any operations
- **Final State**: After exhausting resources
- **Reset**: Returning to initial state
- **Transition**: State machine edge cases

</EDGE_CASE_CATEGORIES>

<TEST_QUALITY_GUIDELINES>

### F.I.R.S.T Principles
- **Fast**: Tests should run quickly (< 10ms each)
- **Isolated**: No dependencies between tests
- **Repeatable**: Same result every time, any environment
- **Self-Validating**: Boolean pass/fail, no manual inspection
- **Timely**: Written before or with the code

### Maintainability
- Use factory functions for test data
- Extract common setup to beforeEach/beforeAll
- One logical assertion per test (ideally)
- Tests should read like documentation
- Avoid logic in tests (no if/else, loops)

### Mocking Guidelines
- Mock external dependencies (APIs, databases, file system)
- Don't mock what you don't own
- Prefer fakes over mocks when possible
- Verify mock interactions for critical paths
- Reset mocks between tests

</TEST_QUALITY_GUIDELINES>

<OUTPUT_FORMAT>
# ðŸ§ª Unit Test Suite

## ðŸ“Š Coverage Analysis

| Category | Scenarios | Coverage Target |
|----------|-----------|-----------------|
| Happy Path | {N} | 100% |
| Error Cases | {N} | 100% |
| Edge Cases | {N} | 90% |
| Boundary Values | {N} | 100% |
| **Total** | **{N}** | **>80%** |

---

## ðŸŽ¯ Test Implementation

### {FunctionName} Tests

```javascript
import { {functionName} } from './{module}';

describe('{FunctionName}', () => {
  // Setup
  beforeEach(() => {
    // Reset mocks, setup test data
  });

  describe('Happy Path', () => {
    it('should {expected behavior} when {condition}', () => {
      // Arrange
      const input = { /* valid input */ };
      const expected = { /* expected result */ };
      
      // Act
      const result = functionName(input);
      
      // Assert
      expect(result).toEqual(expected);
    });
  });

  describe('Edge Cases', () => {
    it.each([
      ['empty input', [], /* expected */],
      ['null input', null, /* expected */],
      ['single element', [1], /* expected */],
    ])('should handle %s', (_, input, expected) => {
      expect(functionName(input)).toEqual(expected);
    });
  });

  describe('Error Handling', () => {
    it('should throw {ErrorType} when {invalid condition}', () => {
      const invalidInput = { /* invalid */ };
      
      expect(() => functionName(invalidInput))
        .toThrow({ErrorType})
        .toThrow('expected error message');
    });
  });
});
```

---

## ðŸ§© Test Data Factories

```javascript
// Factory for creating valid test objects
const create{Entity} = (overrides = {}) => ({
  id: 'test-id-' + Math.random().toString(36),
  name: 'Test Name',
  createdAt: new Date(),
  ...overrides,
});
```

---

## ðŸ”§ Setup Instructions

### Installation
```bash
# Install testing framework (if needed)
npm install --save-dev {framework}

# Install additional utilities
npm install --save-dev {mocking-library} {test-utils}
```

### Configuration
```javascript
// jest.config.js or vitest.config.js
module.exports = {
  testEnvironment: 'node',
  coverageThreshold: {
    global: {
      branches: 80,
      functions: 80,
      lines: 80,
      statements: 80,
    },
  },
};
```

### Running Tests
```bash
# Run all tests
{command} test

# Run with coverage
{command} test --coverage

# Watch mode
{command} test --watch

# Run specific file
{command} test {filename}
```

---

## âœ… Verification Checklist

Before considering tests complete:
- [ ] All happy paths covered
- [ ] All error paths covered
- [ ] Boundary values tested
- [ ] Null/undefined handled
- [ ] Async operations tested
- [ ] Mocks properly reset between tests
- [ ] No test interdependencies
- [ ] Tests run in < 1 second total

---

## ðŸš€ Next Steps

1. **Integration Tests**: Test component interactions
2. **E2E Tests**: Test critical user flows
3. **Property-Based Tests**: Generate random inputs
4. **Mutation Testing**: Verify test quality

</OUTPUT_FORMAT>

<CHAIN_OF_THOUGHT>
Develop tests systematically:
1. Analyze the code to understand its purpose and logic
2. Identify the testing framework from context or imports
3. Map all possible inputs and outputs
4. Identify happy paths (expected normal usage)
5. Identify error cases (invalid inputs, failures)
6. Identify edge cases (boundaries, extremes)
7. Design test structure with describe blocks
8. Write tests following AAA pattern
9. Include parameterized tests for similar scenarios
10. Add factory functions for test data
</CHAIN_OF_THOUGHT>
"""
